{
  "hash": "d08b0043463ca1eeccf1324cf75eec3d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Bayesian–MILP Model for Multi-Tier Supply Chains\"\nsubtitle: \"Integrating Structural Equation Modeling and Customer Satisfaction Optimization\"\nauthor: \"Dr. Daniel A. Olivares Vera\"\ndate: today\n\nabstract: |\n  This paper proposes a Bayesian–MILP model that integrates structural equation\n  modeling (SEM), Bayesian optimization (BO), and mixed-integer linear programming (MILP)\n  for multi-tier supply chains oriented to customer value maximization.\n\nkeywords:\n  - Bayesian Optimization\n  - Structural Equation Modeling\n  - MILP\n  - Supply Chain\n\nformat:\n  html:\n    toc: true\n    number-sections: true\n    code-fold: true\n    code-tools: true\n    theme: cosmo\n    html-math-method:\n      method: mathjax\n      options:\n        tex:\n          tags: ams\n  pdf:\n    toc: true\n    number-sections: true\n    documentclass: article\n    pdf-engine: lualatex\n    geometry: margin=1in\n    fontsize: 12pt\n    include-in-header:\n      text: |\n        \\usepackage{lineno}\n        \\linenumbers\n\nexecute:\n  echo: true\n  warning: false\n  message: false\n  freeze: false\n---\n\n[Download PDF version](index.pdf)\n\n# Introduction\n\nTraditional supply chain optimization focuses primarily on cost minimization\nor efficiency maximization. However, in multi-tier supply networks, upstream\nprocess decisions may strongly influence downstream customer satisfaction\nthrough latent quality constructs.\n\n# Structural Equation Model Specification\n\nThe hierarchical SEM is defined as follows:\n\n$$\n\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta} + \\zeta_i\n$$ {#eq-eta}\n\n$$\nY_{ik} = \\lambda_k \\eta_i + \\varepsilon_{ik}\n$$ {#eq-measure}\n\nwhere:\n\n- $\\mathbf{x}_i$ is the vector of process variables,\n- $\\boldsymbol{\\beta}$ are structural coefficients,\n- $\\lambda_k$ are measurement loadings,\n- $\\zeta_i \\sim \\mathcal{N}(0, \\sigma_\\zeta^2)$,\n- $\\varepsilon_{ik} \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon_k}^2)$.\n\nAs shown in Eq. @eq-eta and Eq. @eq-measure,\n\n\n\nComo se muestra en la Figura @fig-arquitectura, el modelo integra la red multi-eslabón con la capa Bayesiana y la capa de optimización.\n\n![](images/Dan(2026)v2.png){#fig-arquitectura fig-cap=\"Arquitectura general del modelo.\" width=40%}\n\n# Simulated SEM Results\n\n::: {#62f9912d .cell fig-height='4.5' fig-width='7' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nnp.random.seed(42)\n\nout_fig = Path(\"results/figures\")\nout_tab = Path(\"results/tables\")\nout_fig.mkdir(parents=True, exist_ok=True)\nout_tab.mkdir(parents=True, exist_ok=True)\n\nn_products = 200\nn_attributes = 3\nn_process_vars = 5\n\nbeta_true = np.random.normal(0, 0.8, n_process_vars)\nlambda_true = np.array([0.9, 1.1, 0.8])\nsigma_zeta_true = 0.5\nsigma_eps_true = np.array([0.4, 0.3, 0.5])\n\nX = np.random.normal(0, 1, (n_products, n_process_vars))\nzeta = np.random.normal(0, sigma_zeta_true, n_products)\neta = X @ beta_true + zeta\n\nY = np.zeros((n_products, n_attributes))\nfor k in range(n_attributes):\n    eps = np.random.normal(0, sigma_eps_true[k], n_products)\n    Y[:, k] = lambda_true[k] * eta + eps\n\ndfY = pd.DataFrame(Y, columns=[\"Durability\", \"Appearance\", \"Comfort\"])\ncorr = dfY.corr()\n\ncorr.to_csv(out_tab / \"corr_attributes.csv\", index=True)\n\nplt.figure()\nplt.hist(eta, bins=30)\nplt.title(\"Simulated latent quality η\")\nplt.tight_layout()\nplt.savefig(out_fig / \"eta_hist.png\", dpi=200)\nplt.show()\n\ncorr.round(3)\n```\n\n::: {.cell-output .cell-output-display}\n![Distribution of simulated latent quality (η).](index_files/figure-html/cell-2-output-1.png){width=662 height=469 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Durability</th>\n      <th>Appearance</th>\n      <th>Comfort</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Durability</th>\n      <td>1.000</td>\n      <td>0.934</td>\n      <td>0.869</td>\n    </tr>\n    <tr>\n      <th>Appearance</th>\n      <td>0.934</td>\n      <td>1.000</td>\n      <td>0.899</td>\n    </tr>\n    <tr>\n      <th>Comfort</th>\n      <td>0.869</td>\n      <td>0.899</td>\n      <td>1.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# Bayesian SEM Estimation (PyMC)\n\n::: {#64e0ac9b .cell execution_count=2}\n``` {.python .cell-code}\nimport pymc as pm\nimport pytensor.tensor as pt\nimport arviz as az\n\nY_obs = dfY.values\nX_obs = X\n\nn, J = X_obs.shape\nK = Y_obs.shape[1]\n\nwith pm.Model() as sem_model:\n    beta = pm.Normal(\"beta\", mu=0.0, sigma=1.0, shape=J)\n    sigma_zeta = pm.HalfNormal(\"sigma_zeta\", sigma=1.0)\n\n    lam = pm.Normal(\"lambda\", mu=1.0, sigma=0.5, shape=K)\n    sigma_eps = pm.HalfNormal(\"sigma_eps\", sigma=1.0, shape=K)\n\n    eta_latent = pm.Normal(\n        \"eta\",\n        mu=pt.dot(X_obs, beta),\n        sigma=sigma_zeta,\n        shape=n\n    )\n\n    muY = eta_latent[:, None] * lam[None, :]\n\n    pm.Normal(\"Y\", mu=muY, sigma=sigma_eps, observed=Y_obs)\n\n    idata = pm.sample(\n        draws=800,\n        tune=800,\n        chains=2,\n        target_accept=0.9,\n        random_seed=42,\n        progressbar=False\n    )\n\naz.summary(idata, var_names=[\"beta\", \"lambda\", \"sigma_zeta\", \"sigma_eps\"], round_to=3)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>beta[0]</th>\n      <td>0.485</td>\n      <td>0.099</td>\n      <td>0.306</td>\n      <td>0.666</td>\n      <td>0.025</td>\n      <td>0.007</td>\n      <td>14.622</td>\n      <td>85.950</td>\n      <td>1.098</td>\n    </tr>\n    <tr>\n      <th>beta[1]</th>\n      <td>-0.124</td>\n      <td>0.053</td>\n      <td>-0.220</td>\n      <td>-0.020</td>\n      <td>0.005</td>\n      <td>0.002</td>\n      <td>109.609</td>\n      <td>410.665</td>\n      <td>1.018</td>\n    </tr>\n    <tr>\n      <th>beta[2]</th>\n      <td>0.570</td>\n      <td>0.111</td>\n      <td>0.368</td>\n      <td>0.766</td>\n      <td>0.026</td>\n      <td>0.008</td>\n      <td>17.392</td>\n      <td>72.167</td>\n      <td>1.097</td>\n    </tr>\n    <tr>\n      <th>beta[3]</th>\n      <td>1.386</td>\n      <td>0.240</td>\n      <td>0.934</td>\n      <td>1.808</td>\n      <td>0.067</td>\n      <td>0.022</td>\n      <td>13.269</td>\n      <td>45.011</td>\n      <td>1.127</td>\n    </tr>\n    <tr>\n      <th>beta[4]</th>\n      <td>-0.165</td>\n      <td>0.055</td>\n      <td>-0.263</td>\n      <td>-0.061</td>\n      <td>0.008</td>\n      <td>0.001</td>\n      <td>46.888</td>\n      <td>464.406</td>\n      <td>1.036</td>\n    </tr>\n    <tr>\n      <th>lambda[0]</th>\n      <td>0.776</td>\n      <td>0.143</td>\n      <td>0.563</td>\n      <td>1.053</td>\n      <td>0.043</td>\n      <td>0.017</td>\n      <td>14.097</td>\n      <td>45.235</td>\n      <td>1.124</td>\n    </tr>\n    <tr>\n      <th>lambda[1]</th>\n      <td>0.968</td>\n      <td>0.178</td>\n      <td>0.694</td>\n      <td>1.301</td>\n      <td>0.053</td>\n      <td>0.021</td>\n      <td>14.190</td>\n      <td>41.852</td>\n      <td>1.127</td>\n    </tr>\n    <tr>\n      <th>lambda[2]</th>\n      <td>0.709</td>\n      <td>0.132</td>\n      <td>0.499</td>\n      <td>0.957</td>\n      <td>0.039</td>\n      <td>0.015</td>\n      <td>14.669</td>\n      <td>47.337</td>\n      <td>1.121</td>\n    </tr>\n    <tr>\n      <th>sigma_zeta</th>\n      <td>0.627</td>\n      <td>0.117</td>\n      <td>0.418</td>\n      <td>0.828</td>\n      <td>0.031</td>\n      <td>0.010</td>\n      <td>14.858</td>\n      <td>72.299</td>\n      <td>1.113</td>\n    </tr>\n    <tr>\n      <th>sigma_eps[0]</th>\n      <td>0.403</td>\n      <td>0.028</td>\n      <td>0.352</td>\n      <td>0.454</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>695.670</td>\n      <td>1067.204</td>\n      <td>1.006</td>\n    </tr>\n    <tr>\n      <th>sigma_eps[1]</th>\n      <td>0.311</td>\n      <td>0.039</td>\n      <td>0.236</td>\n      <td>0.385</td>\n      <td>0.003</td>\n      <td>0.001</td>\n      <td>156.141</td>\n      <td>200.884</td>\n      <td>1.023</td>\n    </tr>\n    <tr>\n      <th>sigma_eps[2]</th>\n      <td>0.504</td>\n      <td>0.029</td>\n      <td>0.454</td>\n      <td>0.559</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>951.830</td>\n      <td>1221.667</td>\n      <td>1.002</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# Posterior Diagnostics\n\n::: {#c0af60d6 .cell fig-height='5' fig-width='8' execution_count=3}\n``` {.python .cell-code}\naz.plot_trace(idata, var_names=[\"beta\", \"lambda\", \"sigma_zeta\", \"sigma_eps\"])\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![MCMC trace diagnostics.](index_files/figure-html/cell-4-output-1.png){width=1140 height=757 fig-align='center'}\n:::\n:::\n\n\n# Parameter Recovery Check\n\n::: {#a62dc8fd .cell execution_count=4}\n``` {.python .cell-code}\npost = az.summary(idata, var_names=[\"beta\", \"lambda\"], round_to=3)\n\npost_mean_beta = post.loc[[f\"beta[{j}]\" for j in range(J)], \"mean\"].to_numpy()\npost_mean_lam = post.loc[[f\"lambda[{k}]\" for k in range(K)], \"mean\"].to_numpy()\n\ncomparison = pd.DataFrame({\n    \"true\": np.concatenate([beta_true, lambda_true]),\n    \"posterior_mean\": np.concatenate([post_mean_beta, post_mean_lam])\n})\n\ncomparison\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>true</th>\n      <th>posterior_mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.397371</td>\n      <td>0.485</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.110611</td>\n      <td>-0.124</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.518151</td>\n      <td>0.570</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.218424</td>\n      <td>1.386</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.187323</td>\n      <td>-0.165</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.900000</td>\n      <td>0.776</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.100000</td>\n      <td>0.968</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.800000</td>\n      <td>0.709</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#ee1cbdbe .cell fig-height='4.5' fig-width='7' execution_count=5}\n``` {.python .cell-code}\nplt.figure()\nplt.scatter(comparison[\"true\"], comparison[\"posterior_mean\"])\nplt.xlabel(\"True value\")\nplt.ylabel(\"Posterior mean\")\nplt.plot(\n    [comparison[\"true\"].min(), comparison[\"true\"].max()],\n    [comparison[\"true\"].min(), comparison[\"true\"].max()]\n)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Posterior mean vs true parameter values.](index_files/figure-html/cell-6-output-1.png){width=661 height=468 fig-align='center'}\n:::\n:::\n\n\n# Integrated Architecture (Conceptual Diagram)\n\n```{mermaid}\n%%{init: {\n  \"theme\": \"base\",\n  \"themeVariables\": {\n    \"background\": \"#ffffff\",\n    \"primaryColor\": \"#ffffff\",\n    \"primaryTextColor\": \"#111111\",\n    \"primaryBorderColor\": \"#111111\",\n    \"lineColor\": \"#111111\",\n    \"fontSize\": \"12px\"\n  },\n  \"flowchart\": { \"curve\": \"linear\" }\n}}%%\n\nflowchart TB\n\n  subgraph S[\"Red multi-eslabón (tiers)\"]\n    direction TB\n    T3[\"Tier 3\"] --> T2[\"Tier 2\"] --> T1[\"Tier 1\"] --> M[\"Manufacturer\"] --> D[\"CD\"] --> C[\"Customer\"]\n  end\n\n  subgraph B[\"Capa Bayesiana (SEM + BO)\"]\n    direction TB\n    X[\"Procesos / variables<br/>X_s\"] --> ETA[\"Calidad latente η\"]\n    ETA --> Y[\"Criterios Y_{c,k}\"]\n    Y --> SC[\"Satisfacción S_c\"]\n    SC --> U[\"Utilidad económica U\"]\n  end\n\n  S --> X\n  U --> MILP[\"MILP: selección de enlaces z<br/>y flujos q\"]\n  MILP --> S\n```\n\n# Customer Satisfaction and Endogenous Demand\n\nCustomer satisfaction for product $p$ and customer $c$ is defined as:\n\n$$\nS_{c,p} = \\sum_{k=1}^{K} w_{c,k} \\mathbb{E}[Y_{c,p,k}]\n$$ {#eq-satisfaction}\n\nwhere:\n\n- $w_{c,k}$ represents the importance weight of criterion $k$ for customer $c$,\n- $\\sum_{k} w_{c,k} = 1$.\n\nDemand is assumed to be endogenous and driven by satisfaction:\n\n$$\nd_{c,p} = d_{c,p}^{0} + \\alpha_{c,p} S_{c,p}\n$$ {#eq-demand}\n\nwhere:\n\n- $d_{c,p}^{0}$ is the baseline demand,\n- $\\alpha_{c,p}$ measures sensitivity of demand to satisfaction.\n\n\n\n# Economic Utility Function\n\nTotal economic utility is defined as:\n\n$$\nU = \\sum_{c,p} \\left( price_{c,p} \\cdot q_{c,p} \\right)\n+ \\gamma \\sum_{c,p} S_{c,p} q_{c,p}\n- C\n$$ {#eq-utility}\n\nwhere:\n\n- $q_{c,p}$ is the quantity delivered to customer $c$,\n- $\\gamma$ represents the economic impact of satisfaction,\n- $C$ is total supply chain cost.\n\n\n# Cost Structure\n\nTotal cost is composed of:\n\n$$\nC = C^{var} + C^{struct}\n$$ {#eq-cost-total}\n\nVariable cost:\n\n$$\nC^{var} = \\sum_{(i,j),p} c^{arc}_{i,j,p} q_{i,j,p}\n$$ {#eq-cost-var}\n\nStructural cost:\n\n$$\nC^{struct} = \\sum_{(i,j),p} f_{i,j} z_{i,j,p}\n$$ {#eq-cost-struct}\n\n\n# MILP Formulation\n\nThe optimization problem is:\n\n$$\n\\max U\n$$\n\nSubject to:\n\nFlow conservation:\n\n$$\n\\sum_{i} q_{i,j,p} = \\sum_{k} q_{j,k,p}\n$$\n\nCapacity constraints:\n\n$$\n\\sum_{j,p} q_{i,j,p} \\leq Cap_i\n$$\n\nLink activation constraints:\n\n$$\nq_{i,j,p} \\leq M z_{i,j,p}\n$$\n\nBinary structure:\n\n$$\nz_{i,j,p} \\in \\{0,1\\}\n$$\n\n\n\n# Bayesian Optimization Layer (GP + Expected Improvement)\n\nThis section formalizes the Bayesian Optimization (BO) layer used to learn or adapt decision parameters (e.g., customer weights, process targets, or policy parameters) that affect satisfaction and the downstream MILP objective. Let $\\boldsymbol{\\theta}\\in\\Theta\\subset\\mathbb{R}^d$ denote the vector of tunable parameters. Examples include:\n\n- preference weights $\\boldsymbol{\\theta}=\\{w_{c,k}\\}$,\n- process-control targets $\\boldsymbol{\\theta}=\\{x_{s,j}^{\\star}\\}$,\n- economic trade-off parameters $\\boldsymbol{\\theta}=\\{\\gamma,\\alpha_{c,p}\\}$,\n- or any calibration vector that impacts expected satisfaction and utility.\n\nWe define the (black-box) BO objective as the **expected economic utility** induced by $\\boldsymbol{\\theta}$:\n\n$$\nf(\\boldsymbol{\\theta}) \\;=\\; \\mathbb{E}\\Big[\\, U(\\boldsymbol{q}^{\\star}(\\boldsymbol{\\theta}),\\boldsymbol{z}^{\\star}(\\boldsymbol{\\theta});\\,\\boldsymbol{\\theta}) \\,\\Big],\n$$ {#eq-bo-obj}\n\nwhere $(\\boldsymbol{q}^{\\star}(\\boldsymbol{\\theta}),\\boldsymbol{z}^{\\star}(\\boldsymbol{\\theta}))$ is the MILP optimal solution under parameterization $\\boldsymbol{\\theta}$, and the expectation is taken with respect to the Bayesian layer uncertainty (SEM posterior and any stochastic components).\n\n## Gaussian Process Surrogate\n\nAt iteration $n$, we have evaluated $f$ at $\\mathcal{D}_n=\\{(\\boldsymbol{\\theta}_i, y_i)\\}_{i=1}^n$, where:\n\n$$\ny_i \\;=\\; f(\\boldsymbol{\\theta}_i) + \\epsilon_i,\\qquad \\epsilon_i \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2).\n$$ {#eq-bo-noise}\n\nWe place a Gaussian Process prior on $f$:\n\n$$\nf(\\boldsymbol{\\theta}) \\sim \\mathcal{GP}\\big(m(\\boldsymbol{\\theta}),\\,k(\\boldsymbol{\\theta},\\boldsymbol{\\theta}')\\big),\n$$ {#eq-gp-prior}\n\ncommonly using a constant mean $m(\\boldsymbol{\\theta})=m_0$ and an RBF kernel:\n\n$$\nk(\\boldsymbol{\\theta},\\boldsymbol{\\theta}') \\;=\\; \\sigma_f^2 \\exp\\!\\left(\n-\\frac{1}{2}\\sum_{\\ell=1}^{d}\\frac{(\\theta_\\ell-\\theta'_\\ell)^2}{\\rho_\\ell^2}\n\\right).\n$$ {#eq-gp-kernel}\n\nGiven $\\mathcal{D}_n$, the GP posterior at a candidate $\\boldsymbol{\\theta}$ is Gaussian:\n\n$$\nf(\\boldsymbol{\\theta}) \\mid \\mathcal{D}_n \\;\\sim\\; \\mathcal{N}\\!\\big(\\mu_n(\\boldsymbol{\\theta}),\\,\\sigma_n^2(\\boldsymbol{\\theta})\\big),\n$$ {#eq-gp-post}\n\nwith standard expressions:\n\n$$\n\\mu_n(\\boldsymbol{\\theta}) \\;=\\; m(\\boldsymbol{\\theta}) + \\mathbf{k}_n(\\boldsymbol{\\theta})^\\top\n\\big(\\mathbf{K}_n+\\sigma_\\epsilon^2\\mathbf{I}\\big)^{-1}\\big(\\mathbf{y}-\\mathbf{m}_n\\big),\n$$ {#eq-gp-mu}\n\n$$\n\\sigma_n^2(\\boldsymbol{\\theta}) \\;=\\; k(\\boldsymbol{\\theta},\\boldsymbol{\\theta}) -\n\\mathbf{k}_n(\\boldsymbol{\\theta})^\\top\\big(\\mathbf{K}_n+\\sigma_\\epsilon^2\\mathbf{I}\\big)^{-1}\\mathbf{k}_n(\\boldsymbol{\\theta}),\n$$ {#eq-gp-sigma}\n\nwhere $\\mathbf{K}_n=[k(\\boldsymbol{\\theta}_i,\\boldsymbol{\\theta}_j)]_{i,j}$, $\\mathbf{k}_n(\\boldsymbol{\\theta})=[k(\\boldsymbol{\\theta}_1,\\boldsymbol{\\theta}),\\ldots,k(\\boldsymbol{\\theta}_n,\\boldsymbol{\\theta})]^\\top$, $\\mathbf{y}=[y_1,\\ldots,y_n]^\\top$, and $\\mathbf{m}_n=[m(\\boldsymbol{\\theta}_1),\\ldots,m(\\boldsymbol{\\theta}_n)]^\\top$.\n\n## Expected Improvement Acquisition\n\nLet $f_n^{\\max}=\\max_{i\\le n} y_i$ be the best observed value so far. The Expected Improvement (EI) acquisition for maximization is:\n\n$$\n\\mathrm{EI}_n(\\boldsymbol{\\theta}) \\;=\\;\n\\mathbb{E}\\Big[\\max\\big(0,\\, f(\\boldsymbol{\\theta})-f_n^{\\max}-\\xi\\big)\\;\\big|\\;\\mathcal{D}_n\\Big],\n$$ {#eq-ei-def}\n\nwhere $\\xi\\ge 0$ controls exploration. Under the GP posterior \\@eq-gp-post, EI has closed form. Define:\n\n$$\nZ(\\boldsymbol{\\theta}) \\;=\\; \\frac{\\mu_n(\\boldsymbol{\\theta})-f_n^{\\max}-\\xi}{\\sigma_n(\\boldsymbol{\\theta})},\n$$ {#eq-ei-z}\n\nthen:\n\n$$\n\\mathrm{EI}_n(\\boldsymbol{\\theta}) \\;=\\;\n\\big(\\mu_n(\\boldsymbol{\\theta})-f_n^{\\max}-\\xi\\big)\\Phi\\!\\big(Z(\\boldsymbol{\\theta})\\big)\n+\\sigma_n(\\boldsymbol{\\theta})\\phi\\!\\big(Z(\\boldsymbol{\\theta})\\big),\n$$ {#eq-ei-closed}\n\nwith $\\Phi(\\cdot)$ and $\\phi(\\cdot)$ the standard normal CDF and PDF.\n\nThe BO iteration selects the next evaluation point by:\n\n$$\n\\boldsymbol{\\theta}_{n+1} \\;=\\; \\arg\\max_{\\boldsymbol{\\theta}\\in\\Theta}\\;\\mathrm{EI}_n(\\boldsymbol{\\theta}).\n$$ {#eq-bo-next}\n\n## Coupling with the Bayesian SEM and MILP\n\nFor each candidate $\\boldsymbol{\\theta}$ evaluated during BO, the workflow is:\n\n1. **SEM posterior propagation:** draw $(\\boldsymbol{\\beta},\\boldsymbol{\\lambda},\\boldsymbol{\\sigma}) \\sim p(\\cdot\\mid \\text{data})$ and compute $\\mathbb{E}[Y_{c,p,k}\\mid X,\\boldsymbol{\\theta}]$ (or Monte Carlo estimates).\n2. **Satisfaction / demand update:** compute $S_{c,p}(\\boldsymbol{\\theta})$ and any derived demand parameters.\n3. **MILP solve:** solve the MILP to obtain $(\\boldsymbol{q}^{\\star}(\\boldsymbol{\\theta}),\\boldsymbol{z}^{\\star}(\\boldsymbol{\\theta}))$.\n4. **Utility evaluation:** compute $y=f(\\boldsymbol{\\theta})$ as the expected (or Monte Carlo averaged) utility.\n\nThis closes the Bayesian loop: BO learns $\\boldsymbol{\\theta}$ that maximizes the satisfaction-driven expected utility under uncertainty.\n\n\n# Conclusion\n\nThe proposed framework integrates:\n\n- Bayesian Structural Equation Modeling\n- Multi-tier supply chain structure\n- Endogenous satisfaction-driven demand\n- Mixed-integer network optimization\n\nforming a unified architecture for economic utility maximization.\n\n::contentReference[oaicite:0]{index=0}\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}